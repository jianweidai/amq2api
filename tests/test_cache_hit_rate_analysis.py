"""
ç¼“å­˜å‘½ä¸­ç‡åˆ†æå·¥å…·

ç”¨äºè¯Šæ–­ä¸ºä»€ä¹ˆç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®
"""
import json
from typing import Dict, Any, List
from src.processing.cache_manager import CacheManager


def analyze_request_cacheability(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æè¯·æ±‚çš„å¯ç¼“å­˜æ€§
    
    Returns:
        åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
        - has_cache_control: æ˜¯å¦æœ‰ cache_control æ ‡è®°
        - cacheable_content: å¯ç¼“å­˜å†…å®¹
        - token_count: token æ•°é‡
        - issues: å‘ç°çš„é—®é¢˜åˆ—è¡¨
        - suggestions: æ”¹è¿›å»ºè®®åˆ—è¡¨
    """
    cache_manager = CacheManager()
    issues = []
    suggestions = []
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰ cache_control æ ‡è®°
    has_cache_control = False
    
    # æ£€æŸ¥ system prompt
    system = request_data.get("system")
    if system:
        if isinstance(system, str):
            issues.append("system prompt æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸æ”¯æŒ cache_control")
            suggestions.append("å°† system prompt æ”¹ä¸ºæ•°ç»„æ ¼å¼ï¼Œå¹¶æ·»åŠ  cache_control æ ‡è®°")
        elif isinstance(system, list):
            for block in system:
                if isinstance(block, dict) and block.get("cache_control"):
                    has_cache_control = True
                    break
            if not has_cache_control:
                issues.append("system prompt æ˜¯æ•°ç»„æ ¼å¼ï¼Œä½†æ²¡æœ‰ cache_control æ ‡è®°")
                suggestions.append("åœ¨ system prompt çš„æœ€åä¸€ä¸ª block æ·»åŠ  cache_control")
    
    # æ£€æŸ¥ messages
    messages = request_data.get("messages", [])
    message_with_cache = 0
    for message in messages:
        content = message.get("content")
        if isinstance(content, str):
            continue
        elif isinstance(content, list):
            for block in content:
                if isinstance(block, dict) and block.get("cache_control"):
                    has_cache_control = True
                    message_with_cache += 1
                    break
    
    if message_with_cache == 0 and messages:
        issues.append(f"æœ‰ {len(messages)} æ¡æ¶ˆæ¯ï¼Œä½†æ²¡æœ‰ä»»ä½•æ¶ˆæ¯æœ‰ cache_control æ ‡è®°")
        suggestions.append("åœ¨å†å²æ¶ˆæ¯çš„æœ€åä¸€æ¡æ·»åŠ  cache_control æ ‡è®°")
    
    # 2. æå–å¯ç¼“å­˜å†…å®¹
    cacheable_content, token_count = cache_manager.extract_cacheable_content(request_data)
    
    if not cacheable_content:
        issues.append("æ²¡æœ‰æå–åˆ°ä»»ä½•å¯ç¼“å­˜å†…å®¹")
        suggestions.append("ç¡®ä¿è¯·æ±‚ä¸­æœ‰å¸¦ cache_control æ ‡è®°çš„å†…å®¹å—")
    elif token_count < 1024:
        issues.append(f"å¯ç¼“å­˜å†…å®¹å¤ªå°‘ï¼ˆ{token_count} tokensï¼‰ï¼ŒAnthropic è¦æ±‚è‡³å°‘ 1024 tokens")
        suggestions.append("å¢åŠ å¯ç¼“å­˜å†…å®¹çš„é•¿åº¦ï¼Œæˆ–å°†å¤šä¸ªå†…å®¹å—åˆå¹¶")
    
    # 3. æ£€æŸ¥å†…å®¹å˜åŒ–æ€§
    if cacheable_content:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´æˆ³ã€UUID ç­‰åŠ¨æ€å†…å®¹
        dynamic_patterns = [
            ("timestamp", ["timestamp", "time:", "date:", "at 20", "at 19"]),
            ("uuid", ["uuid", "id:", "-", "request_id"]),
            ("random", ["random", "nonce", "session"]),
        ]
        
        for pattern_name, keywords in dynamic_patterns:
            for keyword in keywords:
                if keyword.lower() in cacheable_content.lower():
                    issues.append(f"å¯ç¼“å­˜å†…å®¹ä¸­å¯èƒ½åŒ…å«åŠ¨æ€æ•°æ®ï¼ˆ{pattern_name}ï¼‰")
                    suggestions.append(f"ç§»é™¤æˆ–æ ‡å‡†åŒ–åŠ¨æ€æ•°æ®ï¼ˆ{pattern_name}ï¼‰ï¼Œä½¿å†…å®¹æ›´ç¨³å®š")
                    break
    
    return {
        "has_cache_control": has_cache_control,
        "cacheable_content_length": len(cacheable_content),
        "token_count": token_count,
        "message_count": len(messages),
        "message_with_cache": message_with_cache,
        "issues": issues,
        "suggestions": suggestions,
        "cacheable_content_preview": cacheable_content[:200] if cacheable_content else ""
    }


def simulate_cache_behavior(requests: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ¨¡æ‹Ÿä¸€ç³»åˆ—è¯·æ±‚çš„ç¼“å­˜è¡Œä¸º
    
    Args:
        requests: è¯·æ±‚åˆ—è¡¨
        
    Returns:
        æ¨¡æ‹Ÿç»“æœï¼ŒåŒ…å«å‘½ä¸­ç‡ã€æœªå‘½ä¸­åŸå› ç­‰
    """
    cache_manager = CacheManager(ttl_seconds=86400, max_entries=5000)
    
    results = {
        "total_requests": len(requests),
        "hits": 0,
        "misses": 0,
        "no_cacheable_content": 0,
        "cache_keys": [],
        "duplicate_keys": 0,
    }
    
    seen_keys = set()
    
    for i, request_data in enumerate(requests):
        cacheable_content, token_count = cache_manager.extract_cacheable_content(request_data)
        
        if not cacheable_content:
            results["no_cacheable_content"] += 1
            continue
        
        cache_key = cache_manager.calculate_cache_key(cacheable_content)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„é”®
        if cache_key in seen_keys:
            results["duplicate_keys"] += 1
        else:
            seen_keys.add(cache_key)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_result = cache_manager.check_cache(cache_key, token_count, len(cacheable_content))
        
        if cache_result.is_hit:
            results["hits"] += 1
        else:
            results["misses"] += 1
        
        results["cache_keys"].append({
            "request_index": i,
            "key_preview": cache_key[:32],
            "is_hit": cache_result.is_hit,
            "token_count": token_count,
            "content_length": len(cacheable_content)
        })
    
    # è®¡ç®—å‘½ä¸­ç‡
    total_cacheable = results["hits"] + results["misses"]
    if total_cacheable > 0:
        results["hit_rate"] = results["hits"] / total_cacheable
    else:
        results["hit_rate"] = 0.0
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = cache_manager.get_statistics()
    results["cache_stats"] = {
        "hit_count": stats.hit_count,
        "miss_count": stats.miss_count,
        "hit_rate": stats.hit_rate,
        "total_requests": stats.total_requests
    }
    
    return results


def print_analysis_report(analysis: Dict[str, Any]):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ç¼“å­˜å¯ç”¨æ€§åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print(f"\nâœ“ æ˜¯å¦æœ‰ cache_control æ ‡è®°: {'æ˜¯' if analysis['has_cache_control'] else 'å¦'}")
    print(f"âœ“ å¯ç¼“å­˜å†…å®¹é•¿åº¦: {analysis['cacheable_content_length']} å­—ç¬¦")
    print(f"âœ“ Token æ•°é‡: {analysis['token_count']}")
    print(f"âœ“ æ¶ˆæ¯æ€»æ•°: {analysis['message_count']}")
    print(f"âœ“ å¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯: {analysis['message_with_cache']}")
    
    if analysis['cacheable_content_preview']:
        print(f"\nå¯ç¼“å­˜å†…å®¹é¢„è§ˆ:")
        print(f"  {analysis['cacheable_content_preview']}...")
    
    if analysis['issues']:
        print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for i, issue in enumerate(analysis['issues'], 1):
            print(f"  {i}. {issue}")
    
    if analysis['suggestions']:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, suggestion in enumerate(analysis['suggestions'], 1):
            print(f"  {i}. {suggestion}")
    
    print("\n" + "="*80)


def print_simulation_report(results: Dict[str, Any]):
    """æ‰“å°æ¨¡æ‹ŸæŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ç¼“å­˜è¡Œä¸ºæ¨¡æ‹ŸæŠ¥å‘Š")
    print("="*80)
    
    print(f"\næ€»è¯·æ±‚æ•°: {results['total_requests']}")
    print(f"ç¼“å­˜å‘½ä¸­: {results['hits']}")
    print(f"ç¼“å­˜æœªå‘½ä¸­: {results['misses']}")
    print(f"æ— å¯ç¼“å­˜å†…å®¹: {results['no_cacheable_content']}")
    print(f"é‡å¤çš„ç¼“å­˜é”®: {results['duplicate_keys']}")
    print(f"å‘½ä¸­ç‡: {results['hit_rate']*100:.2f}%")
    
    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    stats = results['cache_stats']
    print(f"  å‘½ä¸­æ¬¡æ•°: {stats['hit_count']}")
    print(f"  æœªå‘½ä¸­æ¬¡æ•°: {stats['miss_count']}")
    print(f"  å‘½ä¸­ç‡: {stats['hit_rate']*100:.2f}%")
    
    print("\n" + "="*80)


# ç¤ºä¾‹ï¼šåˆ†æå…¸å‹çš„ Claude API è¯·æ±‚
if __name__ == "__main__":
    # ç¤ºä¾‹ 1: æ²¡æœ‰ cache_control çš„è¯·æ±‚ï¼ˆå‘½ä¸­ç‡ä½ï¼‰
    print("\nã€ç¤ºä¾‹ 1ã€‘æ²¡æœ‰ cache_control çš„è¯·æ±‚")
    request_without_cache = {
        "model": "claude-sonnet-4.5",
        "max_tokens": 1024,
        "system": "You are a helpful assistant.",
        "messages": [
            {"role": "user", "content": "Hello, how are you?"}
        ]
    }
    analysis1 = analyze_request_cacheability(request_without_cache)
    print_analysis_report(analysis1)
    
    # ç¤ºä¾‹ 2: æœ‰ cache_control çš„è¯·æ±‚ï¼ˆå‘½ä¸­ç‡é«˜ï¼‰
    print("\n\nã€ç¤ºä¾‹ 2ã€‘æœ‰ cache_control çš„è¯·æ±‚")
    request_with_cache = {
        "model": "claude-sonnet-4.5",
        "max_tokens": 1024,
        "system": [
            {
                "type": "text",
                "text": "You are a helpful assistant with access to a large knowledge base.",
                "cache_control": {"type": "ephemeral"}
            }
        ],
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Here is a large document:\n" + "Lorem ipsum " * 500,
                        "cache_control": {"type": "ephemeral"}
                    },
                    {
                        "type": "text",
                        "text": "What is the main topic?"
                    }
                ]
            }
        ]
    }
    analysis2 = analyze_request_cacheability(request_with_cache)
    print_analysis_report(analysis2)
    
    # ç¤ºä¾‹ 3: æ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚çš„ç¼“å­˜è¡Œä¸º
    print("\n\nã€ç¤ºä¾‹ 3ã€‘æ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚çš„ç¼“å­˜è¡Œä¸º")
    
    # åˆ›å»ºä¸€ç³»åˆ—ç›¸ä¼¼çš„è¯·æ±‚ï¼ˆåº”è¯¥æœ‰é«˜å‘½ä¸­ç‡ï¼‰
    requests = []
    base_system = [
        {
            "type": "text",
            "text": "You are a helpful assistant. " + "Context: " * 300,
            "cache_control": {"type": "ephemeral"}
        }
    ]
    
    for i in range(10):
        requests.append({
            "model": "claude-sonnet-4.5",
            "max_tokens": 1024,
            "system": base_system,
            "messages": [
                {"role": "user", "content": f"Question {i}: What is the answer?"}
            ]
        })
    
    simulation_results = simulate_cache_behavior(requests)
    print_simulation_report(simulation_results)
    
    print("\n\nğŸ’¡ æ€»ç»“ï¼š")
    print("1. ç¡®ä¿è¯·æ±‚ä¸­æœ‰ cache_control æ ‡è®°")
    print("2. å¯ç¼“å­˜å†…å®¹åº”è¯¥è‡³å°‘ 1024 tokensï¼ˆAnthropic è¦æ±‚ï¼‰")
    print("3. å¯ç¼“å­˜å†…å®¹åº”è¯¥åœ¨å¤šä¸ªè¯·æ±‚é—´ä¿æŒç¨³å®š")
    print("4. é¿å…åœ¨å¯ç¼“å­˜å†…å®¹ä¸­åŒ…å«åŠ¨æ€æ•°æ®ï¼ˆæ—¶é—´æˆ³ã€UUID ç­‰ï¼‰")
    print("5. ä½¿ç”¨æ•°ç»„æ ¼å¼çš„ system prompt å’Œ message content")
